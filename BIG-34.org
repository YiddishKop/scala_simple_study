# -*- org-export-babel-evaluate: nil -*-
#+PROPERTY: header-args :eval never-export
#+PROPERTY: header-args:python :session Spark SQL-2
#+PROPERTY: header-args:ipython :session Spark SQL-2
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="/home/yiddi/git_repos/YIDDI_org_export_theme/theme/org-nav-theme.css" >
#+HTML_HEAD: <script src="https://hypothes.is/embed.js" async></script>
#+HTML_HEAD: <script type="application/json" class="js-hypothesis-config">
#+HTML_HEAD: <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
#+OPTIONS: html-link-use-abs-url:nil html-postamble:nil html-preamble:t
#+OPTIONS: H:3 num:nil ^:nil _:nil tags:not-in-toc
#+TITLE: Spark SQL-2
#+AUTHOR: yiddishkop
#+EMAIL: [[mailto:yiddishkop@163.com][yiddi's email]]
#+TAGS: {PKGIMPT(i) DATAVIEW(v) DATAPREP(p) GRAPHBUILD(b) GRAPHCOMPT(c)} LINAGAPI(a) PROBAPI(b) MATHFORM(f) MLALGO(m)


I want to plot out by latitude and longitude all of the temperature averages
across the global in data set. so need the take into account each weather
station, so we need latitude and longitude


* how to read different type apart from .csv
  read by RDD, transform it to an column format, and conver to DataSet, then
  build proper DataFrameReader( schema and option) to read it in.

* general process of using DataFrame and SparkSQL
  1. build proper DataFrameReader( schema<-StructType+StructField and option)
  2. wether data file is standard format?
     - no : need respecify how to read and how to store
       - for how to read: drf.option("typeName", "formatDescription")
       - for how to store: drf.schema( StructType(StructField,StructField, ...) )
     - no : need fall back from SparkSession to SparkContext ~ss.SparkContext~
       - using ~sc.textFile(path)~ to read in RDD[String]
       - RDD[String].map { line => spark.sql.Row }: RDD[Row]
     - yes: need ~ss.read.csv/text/json~
  3. create DataFrame from step2 and step1
     - for non-standard fomat file:
       - ss.createDataFrame( schema(StructType+StructField) , RDD[Row] )

 #+NAME: dataset file standard type but wierd details
 #+BEGIN_SRC scala
   // tschema for Temperature data
   val tschema = StructType(Array( // ONLY select and modify the front 4 columns
                              StructField("sid",StringType),
                              StructField("date",DateType),
                              StructField("mtype",StringType),
                              StructField("value",DoubleType)
                            ))
   // data2017 is a DataFrame
   val data2017 = spark.read.schema(tschema).option("dateFormat", "yyyyMMdd").csv("data/2017.csv")
   data2017.show()
 #+END_SRC


  #+NAME: dataset file is of wierd type
  #+BEGIN_SRC scala
    val sschema = StructType(Array(
                               StructField("sid", StringType),
                               StructField("lat", StringType),
                               StructField("lon", StringType),
                               StructField("name", StringType)
                             ))
    val stationRDD = spark.sparkContext.textFile("data/ghcnd-station.txt").map { line =>
      val id = line.substring(0, 11)
      val lat = line.substring(12, 20)
      val lon = line.substring(21, 30)
      val name = line.substring(41, 71)
      Row(id, lat, lon, name)
    }
    val stations = spark.createDataFrame(stationRDD, sschema).cache()
  #+END_SRC

* non standard format data file

  #+BEGIN_EXAMPLE
  >>>> ghcnd-stations.txt
  ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD
  ACW00011647  17.1333  -61.7833   19.2    ST JOHNS
  AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196
  AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194
  AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217

  >>>> readme.txt explaination of the format of ghcnd-stations.txt
  IV. FORMAT OF "ghcnd-stations.txt"
  ------------------------------
  Variable   Columns   Type
  ------------------------------
  ID            1-11   Character
  LATITUDE     13-20   Real
  LONGITUDE    22-30   Real
  ELEVATION    32-37   Real
  STATE        39-40   Character
  NAME         42-71   Character
  GSN FLAG     73-75   Character
  HCN/CRN FLAG 77-79   Character
  WMO ID       81-85   Character
  ------------------------------
  #+END_EXAMPLE
* ~as~ clause

  a shortname version of ~withColumnRename()~, but more convenient.

  #+BEGIN_SRC scala
    val dailyTemp2017 = combinedTemps2017
      .select('sid, 'date, ('tmax + 'tmin)/20 as "tave")

    val stationTemp2017 = dailyTemp2017
      .groupBy('sid)
      .agg(avg('tave) as "tave")
    stationTemp2017.show

  #+END_SRC

  ~.agg(avg('tave) as "tave")~
  ~.select('sid, 'date, ('tmax + 'tmin)/20 as "tave")~

  ~as~ follow after a numeric computation, followed by a String, used to named a new column.

  ~.somemethod( computation as "columnName")~

* DataFrame API
** DataFrame.cache()
   *RDD* has cache() method, because Transform is like a pass-by-name, everytime you refer to a *RDD*, it will compute: *RDD acts like a function name*, so the frequently used *RDD* should be cached in meory or bigger should be in disk; The same for the *DataFrame*, *DataSet* has both kinds of API methods: transform and action.

   #+BEGIN_SRC scala
     val stations = spark.createDataFrame(stationRDD, sschema).cache()
   #+END_SRC
** DataFrame.groupBy(cols: Column*)

   def groupBy( cols: Column* ): *RelationalGroupedDataset*

   class *RelationalGroupedDataset* extends AnyRef

   Similar to ~org.apache.spark.sql.functions~ holding huge number of predefined functions, ~RelationalGroupedDataset~ also has some predefined functions especially for the target of *aggregations on a DataFrame*

   A set of methods for aggregations on a DataFrame, created by *groupBy, cube or rollup (and also pivot)*.

   The main method is the agg function, which has multiple variants. This class also contains some first-order statistics such as mean, sum for convenience.

   #+BEGIN_SRC scala
     val stationTemp2017 = dailyTemp2017
       .groupBy('sid)
       .agg(avg('tave) as "tave")// every group with same 'id compute the averate and as a new column
                                 // same as collection.aggregate and PairRDD.aggregateByKey, these three
                                 // all get the (1 key: 1 value) format for each the value we groupby
   #+END_SRC

   |        sid|              tave|
   |-----------+------------------|
   |AE000041196|            18.975|
   |AEM00041194|             22.65|
   |AEM00041217|20.883333333333333|
   |AEM00041218|20.483333333333334|
   |AGM00060461|           13.6625|
   |ALM00013615|0.7100000000000002|

** RelationalGroupedDataset.agg()
   Refering to the code of this class, find other format to invocate ~agg()~

   Selects the age of the oldest employee and the aggregate expense for each department

   RelationalGroupDataset = rgd
   import org.apache.spark.sql.functions._ = fs

   #+BEGIN_SRC scala
     // method 1 :  rgd.agg + rgd.avg
     df.groupBy("department").agg(
       "age" -> "max",
       "expense" -> "sum"

     // method 2 :  rgd.agg + rgd.avg
     df.groupBy("department").agg(Map(
       "age" -> "max",
       "expense" -> "sum"

     // method 3 :  rgd.agg + fs.avg
     import org.apache.spark.sql.functions._
     df.groupBy("department").agg(max("age"), sum("expense"))
   #+END_SRC

   Note that, the useage manner of ~rgd~ predefined methods are huge different
   from it of ~fs~. Although, the underneath logic both are *apply method to
   each element of column*

   - rgd usage manner: [colName] -> [method]
   - fs  usage manner: [method]([colName])

   for different usage manner, you must declare different package for it, like

   #+BEGIN_SRC scala
   // method 3 :  rgd.agg + fs.avg
   import org.apache.spark.sql.functions._
   df.groupBy("department").agg(max("age"), sum("expense"))
   #+END_SRC

* Memory usage in Spark
  *Don't displaying the values early*

  this is important, if you display the value early, you might have been forcing
  spark SQL to do some stuff with memory intensive, that it might actually be
  able to optimize out if I went through the whole process , clearly we use the
  master mode ~.setMaster("loca[*]")~, which also gives us lot constain.

* RDD and DataSet
  ~DataSet~ has almost all the higher-order methods of ~RDD~,
  ~map/filter/groupBy/flatmap~, etc. they both have same two features:
  *Transform* and *Action* they both store data *distributely* in cluster of
  machines

  #+BEGIN_EXAMPLE
                      +-----------+------------------+
                      |        sid|              tave|
                      +-----------+------------------+
                      |AE000041196|            18.975|
                      |AEM00041194|             22.65|
                      |AEM00041217|20.883333333333333|
                      |AEM00041218|20.483333333333334|
                      |AGM00060461|           13.6625|
                      |ALM00013615|0.7100000000000002|
                      +-----------+------------------+
                                    |
                                    |
          +---------+---------+-----+---+-------------------+
          |         |         |         |                   |
          |         |         |         |                   |
      +---+--+  +---+--+  +---+--+  +---+--+            +---+--+
      | pc1  |  | pc2  |  |  pc3 |  | pc4  |   ......   | pcn  |
      +------+  +------+  +------+  +------+            +------+

  #+END_EXAMPLE

  when you want to ~plot/sort~ the whole DataSet or RDD, you must ~collect()~
  all data from other machines.

* two important funcions set in SPARK SQL
  1. org.apache.spark.sql.functions
  2. org.apache.spark.RelationalGroupedDataset

  they have different basic format, but same underneath logic
  - fs : method(colName)
  - rgd: colName -> method
  - same logic: apply method to each element of column

* one important implicits
  1. SparkSession.implicits

  lots shortname inside of it, like:
  - 'colName -> col
  - $(colName) -> col
  - StrtoCol implicit conversion
